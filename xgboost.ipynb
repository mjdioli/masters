{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils_clean\n",
    "import utils\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 2, 1: 2, 2: 2, 3: 2, 4: 2, 5: 2, 6: 2, 7: 2, 8: 2, 9: 2, 10: 2, 11: 2, 12: 2}\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "test = {i:2 for i in range(13)}\n",
    "\n",
    "try:\n",
    "    with open(Path(\"final_experiment/\"+\"test.json\"), 'w') as f:\n",
    "        json.dump(test, f, indent=1)\n",
    "except:\n",
    "    print(\"COULDNT SAVE SYNTH COMPAS\")\n",
    "print(test)\n",
    "\n",
    "test[\"twenty\"] = 20\n",
    "\n",
    "try:\n",
    "    with open(Path(\"final_experiment/\"+\"test2.json\"), 'w') as f:\n",
    "        json.dump(test, f, indent=1)\n",
    "except:\n",
    "    print(\"COULDNT SAVE SYNTH COMPAS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(response, missing_col, sensitive, dataset = \"recid\", n_runs = 10, robust = True, with_mcar = True):\n",
    "    full_results = {}\n",
    "    for run in tqdm(range(n_runs)):\n",
    "        results = {\"mar\": {metr: {i: [] for i in IMPUTATIONS} for metr in METRICS},\n",
    "                   \"mcar\": {metr: {i: [] for i in IMPUTATIONS} for metr in METRICS}}\n",
    "        np.random.seed(run*13)\n",
    "        if dataset ==\"simple\":\n",
    "            data = utils.load_synthetic(\"simple\")\n",
    "            alpha = SIMPLE_ALPHA\n",
    "        elif dataset ==\"adult\":\n",
    "            data = utils.load_adult()\n",
    "            raise NotImplementedError(\"FOR THE FUTURE\")\n",
    "        elif dataset == \"recid_synth\":\n",
    "            data = utils.load_synthetic()\n",
    "            alpha = RECID_ALPHA\n",
    "        else:\n",
    "            data = utils.load_compas_alt()\n",
    "            alpha = RECID_ALPHA\n",
    "        \n",
    "        train = data[\"train\"]\n",
    "        test = data[\"test\"]\n",
    "        class_0_test = test[test[sensitive] == 0]\n",
    "        class_1_test = test[test[sensitive] == 1]\n",
    "        \n",
    "        noise = np.random.normal(0,0.1,size = len(train))\n",
    "        \n",
    "        for alph, missing_pct in zip(alpha, [0, 5,10,25,50,75]):\n",
    "            #print(missing_pct)\n",
    "            if with_mcar:\n",
    "                data_mcar = utils_clean.data_remover_cat(\n",
    "                        train, missing_col, alph, noise, missing_pct = missing_pct, missingness=\"mcar\", robust = robust)\n",
    "            data_mar = utils_clean.data_remover_cat(\n",
    "                    train, missing_col, alph, noise, missingness=\"mar\", robust = robust)\n",
    "            #print(data_mcar_missing.columns)\n",
    "            #TODO replace MODELS[m] with xgboost and include imputations and without imputations\n",
    "            for imp in IMPUTATIONS:  # , \"reg\"]:\n",
    "                \n",
    "                if imp == \"coldel\" and missing_col == sensitive:\n",
    "                    continue\n",
    "                #print(\"MODEL\",m,\"\\n\", \"IMP\", imp)\n",
    "                #MCAR\n",
    "                # TPR, FPR, TNR, FNR data\n",
    "                #Investigate XGBoost at the fit stage and if data needs to be transformed\n",
    "                if with_mcar:\n",
    "                    predictions_0 = MODELS[m].fit(data_mcar.drop(response, axis=1), data_mcar[response]).predict(\n",
    "                        class_0_test.drop([response, missing_col], axis=1) if imp == \"coldel\" else class_0_test.drop(response, axis=1))\n",
    "                    predictions_1 = MODELS[m].fit(data_mcar.drop(response, axis=1), data_mcar[response]).predict(\n",
    "                        class_1_test.drop([response, missing_col], axis=1) if imp == \"coldel\" else class_0_test.drop(response, axis=1))\n",
    "                    \n",
    "                    \n",
    "                    cf_0 = utils.confusion_matrix(class_0_test[response], predictions_0)\n",
    "                    cf_1 = utils.confusion_matrix(class_1_test[response], predictions_1)\n",
    "                    results[\"mcar\"][\"tpr0\"][m][imp].append(cf_0[\"Predicted true\"][0])\n",
    "                    results[\"mcar\"][\"tpr1\"][m][imp].append(cf_1[\"Predicted true\"][0])\n",
    "                    results[\"mcar\"][\"tnr0\"][m][imp].append(cf_0[\"Predicted false\"][1])\n",
    "                    results[\"mcar\"][\"tnr1\"][m][imp].append(cf_1[\"Predicted false\"][1])\n",
    "                    \n",
    "                    results[m+\"_mcar_\"+imp+\"_\" +\n",
    "                            str(missing_pct)+\"_0\"] = cf_0\n",
    "                    results[m+\"_mcar_\"+imp+\"_\" +\n",
    "                            str(missing_pct)+\"_1\"] = cf_1\n",
    "\n",
    "                    # Fairness metrics\n",
    "                    y_hat = MODELS[m].fit(data_mcar.drop(response, axis=1), data_mcar[response]).predict(\n",
    "                        test.drop([response, missing_col], axis=1) if imp == \"coldel\" else test.drop(response, axis=1))\n",
    "                    results[\"mcar\"][\"spd\"][m][imp].append(\n",
    "                        utils.spd(y_hat, test[sensitive]))\n",
    "                    \n",
    "                    results[\"mcar\"][\"acc\"][m][imp].append(\n",
    "                        utils.accuracy(test[response], y_hat))\n",
    "                    eo = utils.equalised_odds(y_hat, test[sensitive], test[response])\n",
    "                    results[\"mcar\"][\"eosum\"][m][imp].append(eo[\"Y=1\"]+eo[\"Y=0\"])\n",
    "                # TPR, FPR, TNR, FNR data\n",
    "\n",
    "                #MAR\n",
    "                try:\n",
    "                    predictions_0 = MODELS[m].fit(data_mar.drop(response, axis=1), data_mar[response]).predict(\n",
    "                        class_0_test.drop([response, missing_col], axis=1) if imp == \"coldel\" else class_0_test.drop(response, axis=1))\n",
    "                except Exception as e:\n",
    "                    print(\"Exception: \", e)\n",
    "                    print(\"params: \", str(missing_pct)+imp)\n",
    "                    print(\"head: \", data_mar.head())\n",
    "                    print(\"sum: \", data_mar.sum()-len(data_mar))\n",
    "                predictions_1 = MODELS[m].fit(data_mar.drop(response, axis=1), data_mar[response]).predict(\n",
    "                    class_1_test.drop([response, missing_col], axis=1) if imp == \"coldel\" else class_1_test.drop(response, axis=1))\n",
    "                cf_0 = utils.confusion_matrix(class_0_test[response], predictions_0)\n",
    "                cf_1 = utils.confusion_matrix(class_1_test[response], predictions_1)\n",
    "                results[\"mar\"][\"tpr0\"][m][imp].append(cf_0[\"Predicted true\"][0])\n",
    "                results[\"mar\"][\"tpr1\"][m][imp].append(cf_1[\"Predicted true\"][0])\n",
    "                results[\"mar\"][\"tnr0\"][m][imp].append(cf_0[\"Predicted false\"][1])\n",
    "                results[\"mar\"][\"tnr1\"][m][imp].append(cf_1[\"Predicted false\"][1])\n",
    "                    \n",
    "                results[m+\"_mar_\"+imp+\"_\" +\n",
    "                        str(missing_pct)+\"_0\"] = cf_0\n",
    "                results[m+\"_mar_\"+imp+\"_\" +\n",
    "                        str(missing_pct)+\"_1\"] = cf_1\n",
    "\n",
    "                # Fariness metrics\n",
    "                y_hat = MODELS[m].fit(data_mar.drop(response, axis=1), data_mar[response]).predict(\n",
    "                    test.drop([response, missing_col], axis=1) if imp == \"coldel\" else test.drop(response, axis=1))\n",
    "                results[\"mar\"][\"spd\"][m][imp].append(\n",
    "                    utils.spd(y_hat, test[sensitive]))\n",
    "                results[\"mar\"][\"acc\"][m][imp].append(\n",
    "                    utils.accuracy(test[response], y_hat))\n",
    "\n",
    "                eo = utils.equalised_odds(y_hat, test[sensitive], test[response])\n",
    "                results[\"mar\"][\"eosum\"][m][imp].append(eo[\"Y=1\"]+eo[\"Y=0\"])\n",
    "                #TODO add to thesis that missingness was not applied to the test data\n",
    "                #print(imp)\n",
    "        \n",
    "        full_results[str(run)] = results\n",
    "    temp_delta = collections.defaultdict(list)\n",
    "    avg = collections.defaultdict(list)\n",
    "    std = {}\n",
    "    \n",
    "    #Collecting all observations across runs\n",
    "    for miss in [\"mcar\", \"mar\"]:\n",
    "        if not with_mcar and miss ==\"mcar\":\n",
    "            continue\n",
    "        for key in [str(n) for n in range(n_runs)]:\n",
    "            for imp in IMPUTATIONS:\n",
    "                for metric in METRICS:\n",
    "                    temp_delta[miss+\"|\"+metric+\"|\"+\"xgboost\"+\"|\"+imp] += full_results[key][miss][metric][\"xgboost\"][imp]\n",
    "                    avg[miss+\"|\"+metric+\"|\"+\"xgboost\"+\"|\"+imp].append(full_results[key][miss][metric][\"xgboost\"][imp])\n",
    "    \n",
    "    #Averaging\n",
    "    for key, value in avg.items():\n",
    "        avg[key] = [float(n) for n in np.mean(value, axis = 0)]  \n",
    "        std[key] =  [float(n) for n in np.std(value, axis = 0)] \n",
    "          \n",
    "          \n",
    "    try:\n",
    "        utils_clean.save(\"./data/\", \"testresults.pickle\", results)\n",
    "    except Exception as e:\n",
    "        print(\"Couldn't save data with exception: \", e)  \n",
    "    try:\n",
    "        utils_clean.save(\"./data/\", \"testaverages.pickle\", avg)\n",
    "    except Exception as e:\n",
    "        print(\"Couldn't save data with exception: \", e)  \n",
    "    return {\"Full data\": full_results, \"Averaged results\": avg, \"Standard deviation\": std}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f6b5d6a7d9324dc6141b12d74f79997d922c129aaadb2c82c0fa5c003a2c41f2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
